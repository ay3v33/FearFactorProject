{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "532883f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A cyber-attack cut power to thousands in Killeen; investigators found malware in plant network logs and compromised vendor credentials. The mayor set up a task force to strengthen city networks.\n",
      "Killeen Cyber Attack Disrupts Local Power Plant. KILLEEN, TX â€“ On the morning of March 26th, the Killeen Power Plant experienced a cyber-attack that cut power to about 2,300 homes in Killeen. Operators detected unusual network activity at 4:00 a.m. and shut off four feeder circuits as a precaution. Workers are facilitating return of power and expect power to return at around 10:00am. What Happened Systems Hit: Main control server and backup communication lines. Impact: Widespread outages in four\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Folders containing your test files\n",
    "folders = [\"../data/low\", \"../data/med\", \"../data/high\"]\n",
    "\n",
    "# Dictionary to hold all loaded texts\n",
    "texts = {}\n",
    "\n",
    "for folder in folders:\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "                # Use filename without extension as key\n",
    "                key_base = os.path.splitext(filename)[0]\n",
    "                \n",
    "                # Save all keys from the JSON file (like 'text', 'reference_text') inside a sub-dictionary\n",
    "                texts[key_base] = {k: v for k, v in data.items()}\n",
    "\n",
    "# Example access\n",
    "print(texts[\"test8\"][\"text\"][:500])\n",
    "print(texts[\"DocumentForParsing2\"][\"reference_text\"][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d92aedb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# Download NLTK data once\n",
    "# Vader lexicon used for sentiment analysis\n",
    "# nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "260ae10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from lexicon import LEXICON as lexicon\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Keep scores within [-1, 1]\n",
    "def clamp(score):\n",
    "    return max(-1.0, min(1.0, score))\n",
    "\n",
    "# Avoid division by zero if no keyword hits\n",
    "def safe_div(numer, denom):\n",
    "    return numer / denom if denom != 0 else 0.0\n",
    "\n",
    "def analyze_text(text):\n",
    "    fear = stress = morale = trust = 0\n",
    "    hits = []\n",
    "    tokens = text.lower().split()\n",
    "    max_fear = max_stress = max_morale = max_trust = 0\n",
    "\n",
    "    for tok in tokens:\n",
    "        if tok in lexicon:\n",
    "            f, s, m, t = lexicon[tok]\n",
    "            fear += f\n",
    "            stress += s\n",
    "            morale += m\n",
    "            trust += t\n",
    "\n",
    "            # Get max values for scaling\n",
    "            max_fear += max(0, f)\n",
    "            max_stress += max(0, s)\n",
    "            max_morale += max(0, m)\n",
    "            max_trust += max(0, t)\n",
    "\n",
    "            hits.append((tok, (f, s, m, t)))\n",
    "\n",
    "    comp = sia.polarity_scores(text)['compound']\n",
    "    if comp > 0:\n",
    "        morale += comp * 1.0\n",
    "        trust += comp * 0.8\n",
    "        fear -= abs(comp) * 0.3\n",
    "        stress -= abs(comp) * 0.5\n",
    "    elif comp < 0:\n",
    "        fear += abs(comp) * 0.6\n",
    "        stress += abs(comp) * 0.6\n",
    "        morale -= abs(comp) * 0.5\n",
    "        trust -= abs(comp) * 0.3\n",
    "\n",
    "    return \"fear: \" + str(clamp(safe_div(fear, max_fear))) + \", stress: \" + str(clamp(safe_div(stress, max_stress))) + \", morale: \" + str(clamp(safe_div(morale, max_morale))) + \", trust: \" + str(clamp(safe_div(trust, max_trust)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d1dc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1: fear: 0.0, stress: 0.0, morale: 0.0, trust: 0.0\n",
      "test2: fear: 1.0, stress: 1.0, morale: 0.0, trust: 0.0\n",
      "test3: fear: 0.0, stress: 0.0, morale: 0.0, trust: 0.0\n",
      "test4: fear: 0.0, stress: 0.0, morale: 0.0, trust: 0.0\n",
      "test5: fear: 0.0, stress: 0.0, morale: 0.0, trust: 0.0\n",
      "test6: fear: 1.0, stress: 1.0, morale: 0.0, trust: 0.0\n",
      "test7: fear: 0.0, stress: 0.0, morale: 0.0, trust: 0.0\n",
      "test8: fear: 0.0, stress: 0.0, morale: 0.0, trust: 0.0\n",
      "test9: fear: 0.0, stress: 0.0, morale: 0.0, trust: 0.0\n",
      "test10: fear: 1.0, stress: 1.0, morale: 0.0, trust: 0.0\n",
      "test11: fear: 0.0, stress: 0.0, morale: 0.0, trust: 0.0\n",
      "test12: fear: 0.0, stress: 0.0, morale: 0.5610833333333334, trust: 0.47330000000000005\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'test13'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest11:\u001b[39m\u001b[38;5;124m\"\u001b[39m, analyze_text(texts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest11\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest12:\u001b[39m\u001b[38;5;124m\"\u001b[39m, analyze_text(texts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest12\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocumentForParsing:\u001b[39m\u001b[38;5;124m\"\u001b[39m, analyze_text(\u001b[43mtexts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest13\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocumentForParsing2:\u001b[39m\u001b[38;5;124m\"\u001b[39m, analyze_text(texts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest14\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "\u001b[1;31mKeyError\u001b[0m: 'test13'"
     ]
    }
   ],
   "source": [
    "print(\"test1:\", analyze_text(texts[\"test1\"][\"text\"]))\n",
    "print(\"test2:\", analyze_text(texts[\"test2\"][\"text\"]))\n",
    "print(\"test3:\", analyze_text(texts[\"test3\"][\"text\"]))\n",
    "print(\"test4:\", analyze_text(texts[\"test4\"][\"text\"]))\n",
    "print(\"test5:\", analyze_text(texts[\"test5\"][\"text\"]))\n",
    "print(\"test6:\", analyze_text(texts[\"test6\"][\"text\"]))\n",
    "print(\"test7:\", analyze_text(texts[\"test7\"][\"text\"]))\n",
    "print(\"test8:\", analyze_text(texts[\"test8\"][\"text\"]))\n",
    "print(\"test9:\", analyze_text(texts[\"test9\"][\"text\"]))\n",
    "print(\"test10:\", analyze_text(texts[\"test10\"][\"text\"]))\n",
    "print(\"test11:\", analyze_text(texts[\"test11\"][\"text\"]))\n",
    "print(\"test12:\", analyze_text(texts[\"test12\"][\"text\"]))\n",
    "print(\"DocumentForParsing:\", analyze_text(texts[\"DocumentForParsing\"][\"text\"]))\n",
    "print(\"DocumentForParsing2:\", analyze_text(texts[\"DocumentForParsing2\"][\"reference_text\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
